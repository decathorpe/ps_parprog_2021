{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS Parallel Programming / Sheet 08\n",
    "# Fabio Valentini / MN 01018782"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 1\n",
    "\n",
    "```c\n",
    "a[0] = 0;\n",
    "#pragma omp parallel for\n",
    "for (i=1; i<N; i++) {\n",
    "    a[i] = 2.0*i*(i-1);     // S1\n",
    "    b[i] = a[i] - a[i-1];   // S2\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Parallelization\n",
    "\n",
    "No.\n",
    "\n",
    "#### Dependency Analysis\n",
    "\n",
    "The first access to `a[i-1]` in each parallel chunk might depend on uninitialized memory, because it is the last number that is written to `a` in the previous chunk. This is a true dependence $ S_1 \\delta S_2$.\n",
    "\n",
    "#### Potential Parallel Fixes\n",
    "\n",
    "##### 1. Resolve true dependence\n",
    "\n",
    "Splitting the initialization of data in `a` and the calculation of `b` into separate parallel loops would resolve the true dependence, because all elements of `a` would definitely be written before they are read in the second parallel loop.\n",
    "\n",
    "```c\n",
    "a[0] = 0;\n",
    "#pragma omp parallel for\n",
    "for (i=1; i<N; i++) {\n",
    "    a[i] = 2.0*i*(i-1);     // S1\n",
    "}\n",
    "#pragma omp parallel for\n",
    "for (i=1; i<N; i++) {\n",
    "    b[i] = a[i] - a[i-1];   // S2\n",
    "}\n",
    "```\n",
    "\n",
    "##### 2. Change algorithm for calculation of `b`\n",
    "\n",
    "The data in `a` is only dependent on the array index `i`, so it would be possible to calculate the data needed for elements of `b` \"on the fly\" without the helper array `a`, thus completely eliminating the need for a second array and no chance of data races (by solving the recurrence relation):\n",
    "\n",
    "```c\n",
    "#pragma omp parallel for\n",
    "for (i=1; i<N; i++) {\n",
    "    b[i] = 4.0 * i - 4;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 2\n",
    "\n",
    "```c\n",
    "a[0] = 0;\n",
    "#pragma omp parallel\n",
    "{\n",
    "    #pragma omp for nowait\n",
    "    for (i=1; i<N; i++) {\n",
    "        a[i] = 3.0*i*(i+1);    // S1\n",
    "    }\n",
    "    #pragma omp for\n",
    "    for (i=1; i<N; i++) {\n",
    "        b[i] = a[i] - a[i-1];  // S2\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Parallelization\n",
    "\n",
    "No. Same problem as in Snippet 1, because of the `nowait` clause.\n",
    "\n",
    "The true dependence $S_1 \\delta S_2$ is not resolved, because threads of the second loop might be started before threads in the first loop are finished - due to the `nowait` clause, there is no implicit barrier at the end of the for loop.\n",
    "\n",
    "#### Dependency Analysis\n",
    "\n",
    "True dependence $S_1 \\delta S_2$ (see Snippet 1).\n",
    "\n",
    "#### Potential Parallel Fixes\n",
    "\n",
    "##### 1. Dropping `nowait` clause\n",
    "\n",
    "Dropping the `nowait` clause of the first parallel for loop resolves the dependency, because then the implicit barrier at the end of the for loop forces all elements of `a` to be initialized before the second for loop accesses them.\n",
    "\n",
    "##### 2. Change algorithm for calculation of b\n",
    "\n",
    "The recurrence relation can be solved in this case too, leading to a simpler algorithm that does not need array `a` or any synchroniation at all:\n",
    "\n",
    "```c\n",
    "#pragma omp parallel for\n",
    "for (i=1; i<N; i++) {\n",
    "    b[i] = 6.0 * i;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 3\n",
    "\n",
    "```c\n",
    "#pragma omp parallel for\n",
    "for (i=1; i<N; i++) {\n",
    "    x = sqrt(b[i]) - 1;    // S1\n",
    "    a[i] = x*x + 2*x + 1;  // S2\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Parallelization\n",
    "\n",
    "Yes (even if pointers `a` and `b` are aliases).\n",
    "\n",
    "#### Dependency Analysis\n",
    "\n",
    "Array `b` is only read from, array `a` is only written to. There are no potential problems.\n",
    "\n",
    "Even if pointers `a` and `b` are aliases, there is only an antidependence $S_1 \\delta^{-1} S_2$ of array elements with the **same** index `i` in each loop, which is unproblematic.\n",
    "\n",
    "#### Potential Parallel Fixes\n",
    "\n",
    "None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 4\n",
    "\n",
    "```c\n",
    "f = 2;\n",
    "#pragma omp parallel for private(f,x)\n",
    "for (i=1; i<N; i++) {\n",
    "    x = f * b[i];    // S1\n",
    "    a[i] = x - 7;    // S2\n",
    "}\n",
    "a[0] = x; \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Parallelization\n",
    "\n",
    "No. The value of `private(x)` is undefined after the parallel section, so the value of `a[0]` is undefined. The value of `private(f)` is also undefined after entering the parallel region.\n",
    "\n",
    "#### Dependency Analysis\n",
    "\n",
    "There is only an unproblematic antidependence $S_1 \\delta^{-1} S_2$ between array elements with the **same** index `i` of `a` and `b`, so executing in parallel causes no problems.\n",
    "\n",
    "#### Potential Parallel Fixes\n",
    "\n",
    "- use `firstprivate(f)` so `f` will retain its value of `2` from outside the parallel section\n",
    "- use `lastprivate(x)` so `x` will retain its last valid value from inside the parallel region (i.e. from the loop execution with index `i = N-1`)\n",
    "\n",
    "On the other hand, `f` could just be `shared`, as it is not modified inside the loop body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 5\n",
    "\n",
    "```c\n",
    "sum = 0;\n",
    "#pragma omp parallel for\n",
    "for (i=1; i<N; i++) {\n",
    "    sum = sum + b[i];    // S1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Parallelization\n",
    "\n",
    "No. There are possible data races during the update of `sum`.\n",
    "\n",
    "#### Dependency Analysis\n",
    "\n",
    "There is a true dependence $S_1 \\delta S_1$ and an antidependence $S_1 \\delta^{-1} S_1$ between different loop executions, as the shared `sum` is both written to and read from every time, regardless of loop counter, making all executions of the loop body depend on all others.\n",
    "\n",
    "#### Potential Parallel Fixes\n",
    "\n",
    "##### 1. use local partial sums and synchronize access to global `sum` (see exercises from week 01)\n",
    "\n",
    "```c\n",
    "sum = 0;\n",
    "\n",
    "#pragma omp parallel\n",
    "{\n",
    "    _ local_sum = 0;\n",
    "\n",
    "    #pragma omp for\n",
    "    for (i=1; i<N; i++) {\n",
    "        local_sum = local_sum + b[i];  // S1\n",
    "    }\n",
    "\n",
    "    #pragma omp critical\n",
    "    sum += local_sum;\n",
    "}\n",
    "```\n",
    "\n",
    "##### 2. Use reduction functionality provided by OpenMP\n",
    "\n",
    "```c\n",
    "sum = 0;\n",
    "#pragma omp parallel for reduction (+: sum)\n",
    "for (i=1; i<N; i++) {\n",
    "    sum = sum + b[i];    // S1\n",
    "}\n",
    "```\n",
    "\n",
    "This version has the benefit of being very simple, and possibly benefiting from an optimized summation algorithm provided by OpenMP, but should otherwise translate to similar code as fixed snippet 1, using local partial sums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code of `analysis.c` was annotated with GCC 8.2.0 compiler output when compiling with `-O2 -ftree-vectorize -fopt-info-vec-all`. Note that compiler output was in reverse order (last loop in source file considered first), so the order in the annotated source code does not directly correspond to the compiler output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "#include <stdio.h>\n",
    "\n",
    "#define SIZE 1024\n",
    "\n",
    "int main(int argc, char** argv) {\n",
    "\n",
    "        int a[SIZE];\n",
    "        int b[SIZE];\n",
    "\n",
    "        for(int i = 0; i < SIZE; ++i) {\n",
    "                a[i] = argc;\n",
    "        }\n",
    "```\n",
    "\n",
    "    Analyzing loop at analysis.c:10\n",
    "    [...]\n",
    "    analysis.c:10:9: note: step: 1,  init: 0\n",
    "    [...]\n",
    "    analysis.c:10:9: note: vectype: vector(4) int\n",
    "    analysis.c:10:9: note: nunits = 4\n",
    "    [...]\n",
    "    analysis.c:10:9: note: vectorization_factor = 4, niters = 1024\n",
    "    [...]\n",
    "    analysis.c:10:9: note: recording new base alignment for &a\n",
    "    analysis.c:10:9: note:   alignment:    16\n",
    "    analysis.c:10:9: note:   misalignment: 0\n",
    "    [...]\n",
    "    analysis.c:10:9: note: Cost model analysis: \n",
    "      Vector inside of loop cost: 12\n",
    "      Vector prologue cost: 4\n",
    "      Vector epilogue cost: 0\n",
    "      Scalar iteration cost: 12\n",
    "      Scalar outside cost: 0\n",
    "      Vector outside cost: 4\n",
    "      prologue iterations: 0\n",
    "      epilogue iterations: 0\n",
    "      Calculated minimum iters for profitability: 1\n",
    "    analysis.c:10:9: note:   Runtime profitability threshold = 4\n",
    "    analysis.c:10:9: note:   Static estimate profitability threshold = 4\n",
    "    [...]\n",
    "    analysis.c:10:9: note: New loop exit condition: if (ivtmp_66 < 256)\n",
    "    analysis.c:10:9: note: LOOP VECTORIZED\n",
    "\n",
    "- `step: 1,  init: 0`: corresponds to loop variable initialization and update pattern\n",
    "- no dependence distance / information: `a` is only written to, and the written values are constants\n",
    "- `vectorization_factor = 4, niters = 1024`: SIMD instructions for 4 values are chosen, 1024 loop iterations calculated\n",
    "- `recording new base alignment for &a` and `alignment: 16` seems to make sure memory access is correctly aligned for SIMD operations\n",
    "- `Cost model analysis`: compiler includes a const-benefit analysis of vectorizing the loop (probably based on heuristics and CPU specs / instruction \"costs\", if available); in this case, the benefit apparently outweighs the overhead even for very small arrays (>= 4 elements)\n",
    "- `New loop exit condition: if (ivtmp_66 < 256)`: corresponds to 1024 / 4 loop iterations for transformed loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "        for(int i = 0; i < SIZE; ++i) {\n",
    "                b[i] = a[i];\n",
    "        }\n",
    "```\n",
    "\n",
    "    Analyzing loop at analysis.c:14\n",
    "    [...]\n",
    "    analysis.c:14:9: note: Access function of PHI: {0, +, 1}_2\n",
    "    analysis.c:14:9: note: step: 1,  init: 0\n",
    "    [...]\n",
    "    analysis.c:14:9: note: vectype: vector(4) int\n",
    "    analysis.c:14:9: note: nunits = 4\n",
    "    [...]\n",
    "    analysis.c:14:9: note: vectorization_factor = 4, niters = 1024\n",
    "    [...]\n",
    "    analysis.c:14:9: note: recording new base alignment for &a\n",
    "    analysis.c:14:9: note:   alignment:    16\n",
    "    analysis.c:14:9: note:   misalignment: 0\n",
    "    [...]\n",
    "    analysis.c:14:9: note: recording new base alignment for &b\n",
    "    analysis.c:14:9: note:   alignment:    16\n",
    "    analysis.c:14:9: note:   misalignment: 0\n",
    "    [...]\n",
    "    analysis.c:14:9: note: Cost model analysis: \n",
    "      Vector inside of loop cost: 24\n",
    "      Vector prologue cost: 0\n",
    "      Vector epilogue cost: 0\n",
    "      Scalar iteration cost: 24\n",
    "      Scalar outside cost: 0\n",
    "      Vector outside cost: 0\n",
    "      prologue iterations: 0\n",
    "      epilogue iterations: 0\n",
    "      Calculated minimum iters for profitability: 0\n",
    "    analysis.c:14:9: note:   Runtime profitability threshold = 4\n",
    "    analysis.c:14:9: note:   Static estimate profitability threshold = 4\n",
    "    [...]\n",
    "    analysis.c:14:9: note: New loop exit condition: if (ivtmp_60 < 256)\n",
    "    analysis.c:14:9: note: LOOP VECTORIZED\n",
    "\n",
    "- `step: 1,  init: 0`: corresponds to loop variable initialization and update pattern\n",
    "- no dependence distance / information: `a` and `b` are different arrays and no aliases (can be checked statically in this case)\n",
    "- `vectorization_factor = 4, niters = 1024`: SIMD instructions for 4 values are chosen, 1024 loop iterations calculated\n",
    "- `recording new base alignment for &a / &b` and `alignment: 16` seems to make sure memory access is correctly aligned for SIMD operations\n",
    "- `Cost model analysis`: compiler includes a const-benefit analysis of vectorizing the loop (probably based on heuristics and CPU specs / instruction \"costs\", if available); in this case, the benefit apparently outweighs the overhead even for very small arrays (>= 4 elements)\n",
    "- `New loop exit condition: if (ivtmp_60 < 256)`: corresponds to 1024 / 4 loop iterations for transformed loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "        for(int i = 4; i < SIZE; ++i) {\n",
    "                a[i-4] = a[i];\n",
    "        }\n",
    "```\n",
    "\n",
    "    Analyzing loop at analysis.c:18\n",
    "    [...]\n",
    "    analysis.c:18:9: note: step: 1,  init: 4\n",
    "    [...]\n",
    "    analysis.c:18:9: note: dependence distance  = 4.\n",
    "    analysis.c:18:9: note: dependence distance negative.\n",
    "    [...]\n",
    "    analysis.c:18:9: note: get vectype for scalar type:  int\n",
    "    analysis.c:18:9: note: vectype: vector(4) int\n",
    "    [...]\n",
    "    analysis.c:18:9: note: vectorization_factor = 4, niters = 1020\n",
    "    [...]\n",
    "    analysis.c:18:9: note: accesses have the same alignment: a[i_51] and a[_2]\n",
    "    analysis.c:18:9: note: recording new base alignment for &a\n",
    "    analysis.c:18:9: note:   alignment:    16\n",
    "    analysis.c:18:9: note:   misalignment: 0\n",
    "    [...]\n",
    "    analysis.c:18:9: note: Cost model analysis: \n",
    "      Vector inside of loop cost: 24\n",
    "      Vector prologue cost: 0\n",
    "      Vector epilogue cost: 0\n",
    "      Scalar iteration cost: 24\n",
    "      Scalar outside cost: 0\n",
    "      Vector outside cost: 0\n",
    "      prologue iterations: 0\n",
    "      epilogue iterations: 0\n",
    "      Calculated minimum iters for profitability: 0\n",
    "    analysis.c:18:9: note:   Runtime profitability threshold = 4\n",
    "    analysis.c:18:9: note:   Static estimate profitability threshold = 4\n",
    "    [...]\n",
    "    analysis.c:18:9: note: New loop exit condition: if (ivtmp_14 < 255)\n",
    "    analysis.c:18:9: note: LOOP VECTORIZED\n",
    "\n",
    "Some statements in this compiler output snippet are particularly interesting:\n",
    "\n",
    "- `step: 1,  init: 4`: corresponds to loop variable initialization and update pattern\n",
    "- `dependence distance  = 4`: corresponds to `a[i-4]` and `a[i]` access pattern\n",
    "- `dependence distance negative`: array elements are only ever read before they are written to\n",
    "- `vectorization_factor = 4, niters = 1020`: SIMD instructions for 4 values are chosen, 1020 loop iterations calculated\n",
    "- `recording new base alignment for &a` and `alignment: 16` seems to make sure memory access is correctly aligned for SIMD operations\n",
    "- `Cost model analysis`: compiler includes a const-benefit analysis of vectorizing the loop (probably based on heuristics and CPU specs / instruction \"costs\", if available); in this case, the benefit apparently outweighs the overhead even for very small arrays (>= 4 elements)\n",
    "- `New loop exit condition: if (ivtmp_14 < 255)`: corresponds to 1020 / 4 loop iterations for transformed loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "        for(int i = 1; i < SIZE-1; ++i) {\n",
    "                a[i] = a[i%argc];\n",
    "        }\n",
    "```\n",
    "\n",
    "    Analyzing loop at analysis.c:22\n",
    "    [...]\n",
    "    vector(4) int\n",
    "    analysis.c:22:9: note: not vectorized: not suitable for gather load _5 = a[_4];\n",
    "    analysis.c:22:9: note: bad data references.\n",
    "\n",
    "Looks like GCC does not vectorize this loop because of unpredictable array access patterns (because they depend on the number of command line arguments, they cannot be analyzed statically)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "        // output data to prevent compiler from removing any code\n",
    "        for(int i = 0; i < SIZE; ++i) {\n",
    "                printf(\"%d \", a[i]);\n",
    "                printf(\"%d \", b[i]);\n",
    "        }\n",
    "```\n",
    "\n",
    "    Analyzing loop at analysis.c:27\n",
    "    [...]\n",
    "    analysis.c:27:9: note: not vectorized: loop contains function calls or data references that cannot be analyzed\n",
    "\n",
    "It makes sense that the compiler would not vectorize this loop. I assume that it is never safe to vectorize calls to functions that write to file descriptors (in this case, `stdout`), as that would mess up the order in which data is written, even if the `printf` implementation were thread-safe and would flush buffers after every write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "        printf(\"\\n\");\n",
    "\n",
    "        return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in addition to dependence analysis (signed distance of memory accesses, etc.), the compiler also included procedures to make sure data is properly aligned in memory for SIMD instructions, and a cost-benefit analysis for vectorizing loops vs. not vectorizing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 1\n",
    "\n",
    "```c\n",
    "void copy(double* x, double* y) {\n",
    "    for(int i = 0; i < 1024; i++) {\n",
    "        x[i] = y[i];\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can not be safely parallelized by the compiler. `x` and `y` could be aliases, or could be pointing to overlapping memory regions, in which case there would be dependencies between read and write accesses in the overlapping region.\n",
    "\n",
    "If the programmer can verify that `x` and `y` will not be pointing to overlapping memory regions, this can be safely parallelized; either by reasoning that the function is always called with valid arguments, or by running checks before attempting to copy:\n",
    "\n",
    "```c\n",
    "#define SIZE 1024\n",
    "\n",
    "int copy(double* x, double* y) {\n",
    "    if (x == y)\n",
    "        // pointless\n",
    "        return -1;\n",
    "\n",
    "    if ((x > y && (y+SIZE >= x)) || (y > x && (x+SIZE >= y)))\n",
    "        // overlapping memory regions\n",
    "        return -1;\n",
    "\n",
    "    #pragma omp parallel for\n",
    "    for(int i = 0; i < SIZE; i++) {\n",
    "        x[i] = y[i];\n",
    "    }\n",
    "\n",
    "    // successful copy\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 2\n",
    "\n",
    "Normalize the following loop nest:\n",
    "\n",
    "```c\n",
    "for (int i=4; i<=N; i+=7) {\n",
    "    for (int j=0; j<=N; j+=3) {\n",
    "        A[i] = 0;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `i1`: $L_i = 4$, $U_i = N$, $S_i = 7$\n",
    "- `i2`: $L_j = 0$, $U_j = N$, $S_j = 3$\n",
    "- $i \\rightarrow i_1 \\cdot S_i - S_i + L_i = i_1 \\cdot 7 - 7 + 4 = i_1 \\cdot 7 - 3$\n",
    "- $j \\rightarrow i_2 \\cdot S_j - S_j + L_j = i_2 \\cdot 3 - 3 + 0 = i_2 \\cdot 3 - 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $L_{i_1} = 1$, $S_{i_1} = 1$\n",
    "- $U_{i_1} = (U_i - L_i + S_i) / S_i = (N-4)/7 + 1$\n",
    "- $L_{i_2} = 1$, $S_{i_2} = 1$\n",
    "- $U_{i_2} = (U_j - L_j + S_j) / S_j = N/3 + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "for (int i1 = 1; i1 <= ((N-4)/7)+1; i1++) {\n",
    "    for (int i2 = 1; i2 <= (N/3)+1; i2++) {\n",
    "        A[i1*7-3] = 0;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippet 3\n",
    "\n",
    "Dependency analysis, distance vectors, possible parallelization:\n",
    "\n",
    "```c\n",
    "for(int i = 1; i < N; i++) {\n",
    "    for(int j = 1; j < M; j++) {\n",
    "        for(int k = 1; k < L; k++) {\n",
    "            a[i+1][j][k-1] = a[i][j][k] + 5;    // S1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "This loop next is already normalized, because `i<N` and `i<=N-1` etc. are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a true dependence of statement `S1` on itself ($S_1 \\delta S_1$, due to loop variable `i`), and an antidependence of `S1` on itself ($S_1 \\delta^{-1} S_1$, due to loop variable `k`). For the true dependence, the distance vector evaluates to a constant $(+1, 0, -1)$, and the direction vector evaluates to $(<,=,>)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outermost loop cannot be parallelized at all, due to the true dependence with distance 1. The innermost loop cannot be parallelized at all due to an antidependence with distance 1 (this could be resolved by using a temporary copy of the whole array `a`, which would allow to write elements \"before they are read\", because the \"original\" element can still be read from the temporary copy instead).\n",
    "\n",
    "However, the middle loop can be safely parallelized, because memory locations of elements that are written to are always distinct from memory locations of elements that are read from (since the most-significant array index `i` is different), for example:\n",
    "\n",
    "```c\n",
    "for(int i = 1; i < N; i++) {\n",
    "    # pragma omp parallel for\n",
    "    for(int j = 1; j < M; j++) {\n",
    "        for(int k = 1; k < L; k++) {\n",
    "            a[i+1][j][k-1] = a[i][j][k] + 5;    // S1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Whether this parallelization provides a good performance improvement will probably depend on the size of the constants `N`, `M`, and `L`. If `M` is small compared to the other dimensions, then the benefit will probably be small. If `M` is the biggest dimension, then this parallelization should provide a decent speedup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
